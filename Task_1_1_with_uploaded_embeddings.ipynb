{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXjQQ6GJLMYo",
    "outputId": "9277afc7-e414-46d1-d966-4795cf01e44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wLwDNmBAt5Ux",
    "outputId": "76b36e1f-635a-4e34-82d9-a1833a1293ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval1.tar.gz' -> '/content/Data1Embd/eval1.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval3.tar.gz' -> '/content/Data1Embd/eval3.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval2.tar.gz' -> '/content/Data1Embd/eval2.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval4.tar.gz' -> '/content/Data1Embd/eval4.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval10.tar.gz' -> '/content/Data1Embd/eval10.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval8.tar.gz' -> '/content/Data1Embd/eval8.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval5.tar.gz' -> '/content/Data1Embd/eval5.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train5.tar.gz' -> '/content/Data1Embd/train5.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval6.tar.gz' -> '/content/Data1Embd/eval6.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval9.tar.gz' -> '/content/Data1Embd/eval9.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train1.tar.gz' -> '/content/Data1Embd/train1.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/eval7.tar.gz' -> '/content/Data1Embd/eval7.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train6.tar.gz' -> '/content/Data1Embd/train6.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train2.tar.gz' -> '/content/Data1Embd/train2.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train4.tar.gz' -> '/content/Data1Embd/train4.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train10.tar.gz' -> '/content/Data1Embd/train10.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train3.tar.gz' -> '/content/Data1Embd/train3.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train8.tar.gz' -> '/content/Data1Embd/train8.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train9.tar.gz' -> '/content/Data1Embd/train9.tar.gz'\n",
      "'/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd/train7.tar.gz' -> '/content/Data1Embd/train7.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!cp -av '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/Data1Embd' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ__nEHoJBS1",
    "outputId": "72021a2f-d3a6-4f01-ae1b-1b7692d7d790"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-f16c88948b4b>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All eval data shape: (10, 2500, 32, 32, 3)\n",
      "All eval targets shape: (10, 2500)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# List of paths to your dataset files\n",
    "dataset_paths = [\n",
    "     '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth',\n",
    "]\n",
    "\n",
    "# Initialize lists to store data and targets from each dataset\n",
    "all_eval_data = []\n",
    "all_eval_targets = []\n",
    "\n",
    "# Loop through each dataset path and load the data and targets\n",
    "for path in dataset_paths:\n",
    "    dataset = torch.load(path)\n",
    "    data = dataset['data']\n",
    "    targets = dataset['targets']\n",
    "\n",
    "    # Append to the lists\n",
    "    all_eval_data.append(data)\n",
    "    all_eval_targets.append(targets)\n",
    "all_eval_data = np.array(all_eval_data)\n",
    "all_eval_targets = np.array(all_eval_targets)\n",
    "\n",
    "# Print the shape of the concatenated tensors to verify\n",
    "print(\"All eval data shape:\", all_eval_data.shape)\n",
    "print(\"All eval targets shape:\", all_eval_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JM3M__mKaha",
    "outputId": "79666a8d-be06-4060-e52a-71a5e3f13a30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-df2b9b731a13>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All train data shape: (10, 2500, 32, 32, 3)\n",
      "All train targets shape: (1, 2500)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# List of paths to dataset files\n",
    "dataset_paths = [\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/1_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/2_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/3_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/4_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/5_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/6_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/7_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/8_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/9_train_data.tar.pth',\n",
    "    '/content/drive/MyDrive/ml_asg_2/ml-mini-project2/dataset/part_one_dataset/train_data/10_train_data.tar.pth',\n",
    "]\n",
    "\n",
    "# Initialize lists to store data and targets from each dataset\n",
    "all_train_data = []\n",
    "all_train_targets = []\n",
    "i=0\n",
    "# Loop through each dataset path and load the data and targets\n",
    "for path in dataset_paths:\n",
    "    dataset = torch.load(path)\n",
    "    data = dataset['data']\n",
    "    if i==0:\n",
    "      targets = dataset['targets']\n",
    "      i+=1\n",
    "      all_train_targets.append(targets)\n",
    "    # Append to the lists\n",
    "    all_train_data.append(data)\n",
    "\n",
    "all_train_data = np.array(all_train_data)\n",
    "all_train_targets = np.array(all_train_targets)\n",
    "\n",
    "# Print the shape of the concatenated tensors to verify\n",
    "print(\"All train data shape:\", all_train_data.shape)\n",
    "print(\"All train targets shape:\", all_train_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XSn5y8Mg61yc",
    "outputId": "d32a2dab-0170-42a6-a36b-38aa9f7f603f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the archive: ['train1.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train2.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train3.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train4.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train5.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train6.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train7.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train8.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train9.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['train10.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval1.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval2.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval3.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval4.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval5.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval6.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval7.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval8.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval9.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "Files in the archive: ['eval10.npy']\n",
      "Data from the .npy file:\n",
      "(2500, 512)\n",
      "(10, 2500, 512)\n",
      "(10, 2500, 512)\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "dataSet_1_allTrainEmbed = []\n",
    "dataSet_1_allEvalEmbed = []\n",
    "\n",
    "trainDataSetPaths = [\n",
    "    '/content/Data1Embd/train1.tar.gz',\n",
    "    '/content/Data1Embd/train2.tar.gz',\n",
    "    '/content/Data1Embd/train3.tar.gz',\n",
    "    '/content/Data1Embd/train4.tar.gz',\n",
    "    '/content/Data1Embd/train5.tar.gz',\n",
    "    '/content/Data1Embd/train6.tar.gz',\n",
    "    '/content/Data1Embd/train7.tar.gz',\n",
    "    '/content/Data1Embd/train8.tar.gz',\n",
    "    '/content/Data1Embd/train9.tar.gz',\n",
    "    '/content/Data1Embd/train10.tar.gz',\n",
    "]\n",
    "\n",
    "evalDataSetPaths = [\n",
    "    '/content/Data1Embd/eval1.tar.gz',\n",
    "    '/content/Data1Embd/eval2.tar.gz',\n",
    "    '/content/Data1Embd/eval3.tar.gz',\n",
    "    '/content/Data1Embd/eval4.tar.gz',\n",
    "    '/content/Data1Embd/eval5.tar.gz',\n",
    "    '/content/Data1Embd/eval6.tar.gz',\n",
    "    '/content/Data1Embd/eval7.tar.gz',\n",
    "    '/content/Data1Embd/eval8.tar.gz',\n",
    "    '/content/Data1Embd/eval9.tar.gz',\n",
    "    '/content/Data1Embd/eval10.tar.gz',\n",
    "]\n",
    "\n",
    "def loadEmbeddings(tar_gz_path,bigArray):\n",
    "\n",
    "  # Extract the .npy file\n",
    "  with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "      # List all files in the tar.gz archive\n",
    "      tar_members = tar.getnames()\n",
    "      print(\"Files in the archive:\", tar_members)\n",
    "\n",
    "      # Find and extract the .npy file\n",
    "      for member in tar_members:\n",
    "          if member.endswith(\".npy\"):\n",
    "              npy_file_name = member\n",
    "              tar.extract(member, \"extracted_files\")  # Extract to a folder\n",
    "              break\n",
    "      else:\n",
    "          raise FileNotFoundError(\"No .npy file found in the archive.\")\n",
    "\n",
    "  # Path to the extracted .npy file\n",
    "  extracted_npy_path = f\"extracted_files/{npy_file_name}\"\n",
    "\n",
    "  # Load the .npy file\n",
    "  pynData = np.load(extracted_npy_path, allow_pickle=True)  # Set allow_pickle if needed\n",
    "\n",
    "  # Print the loaded data\n",
    "  print(\"Data from the .npy file:\")\n",
    "  print(pynData.shape)\n",
    "\n",
    "  bigArray.append(pynData);\n",
    "\n",
    "for path in trainDataSetPaths:\n",
    "  loadEmbeddings(path,dataSet_1_allTrainEmbed)\n",
    "\n",
    "for path in evalDataSetPaths:\n",
    "  loadEmbeddings(path,dataSet_1_allEvalEmbed)\n",
    "\n",
    "dataSet_1_allTrainEmbed = np.array(dataSet_1_allTrainEmbed)\n",
    "dataSet_1_allEvalEmbed = np.array(dataSet_1_allEvalEmbed)\n",
    "print(dataSet_1_allTrainEmbed.shape)\n",
    "print(dataSet_1_allEvalEmbed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "e5SKxUP2w97Q"
   },
   "outputs": [],
   "source": [
    "def mean(dataSet11Emb , targets):\n",
    "  feature_vectors = [[] for _ in range(10)]\n",
    "  # Iterate through the input arrays to perform inference and collect feature vectors\n",
    "  for i in range(targets.shape[0]):\n",
    "      dropout_output = dataSet11Emb[i]\n",
    "      class_label = targets[i]\n",
    "\n",
    "      # Store the feature vector in the corresponding class list\n",
    "      feature_vectors[class_label].append(dropout_output)\n",
    "\n",
    "  # Calculate mean and variance for each class\n",
    "  mean = {}\n",
    "  for class_idx in range(10):\n",
    "      if feature_vectors[class_idx]:  # Check if there are feature vectors for this class\n",
    "          class_features = np.array(feature_vectors[class_idx])\n",
    "          mean_vector = np.mean(class_features, axis=0)\n",
    "          mean[class_idx] = (mean_vector)\n",
    "      else:\n",
    "          mean[class_idx] = (None, None)  # No samples for this class\n",
    "\n",
    "  # Print the results\n",
    "  '''\n",
    "  for class_idx in range(10):\n",
    "      mean, variance = mean_variance[class_idx]\n",
    "      print(f\"Class {class_idx}: Mean = {mean}, Variance = {variance}\")'''\n",
    "\n",
    "  # Remove the hook\n",
    "\n",
    "  prototypes = {}\n",
    "  for class_idx in range(10):\n",
    "      mean_vector= mean[class_idx]\n",
    "      prototypes[class_idx] = mean_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Ip_IsnWaxZ6S"
   },
   "outputs": [],
   "source": [
    "prototypes1_new  = mean(dataSet_1_allTrainEmbed[0] , all_train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ItihYzI7I8V"
   },
   "outputs": [],
   "source": [
    "# function to calculate euclidean distance\n",
    "def distance_euc(x, mean):\n",
    "    diff = x - mean\n",
    "\n",
    "    return np.sqrt(diff @ diff.T)\n",
    "def classify_with_prototypes(feature_vector, prototypes):\n",
    "    distances = {}\n",
    "    for class_idx, prototype in prototypes.items():\n",
    "        if prototype is not None:  # Check if the prototype exists\n",
    "            distance = distance_euc(feature_vector, prototype)\n",
    "            distances[class_idx] = distance\n",
    "\n",
    "    # Return the class with the minimum distance\n",
    "    predicted_class = min(distances, key=distances.get)\n",
    "    return predicted_class\n",
    "\n",
    "def pred(i, j, prototypes ):\n",
    "  predicted_class = classify_with_prototypes(dataSet_1_allEvalEmbed[i][j], prototypes)\n",
    "  return predicted_class\n",
    "\n",
    "\n",
    "def pred1(i, j, prototypes ):\n",
    "  predicted_class = classify_with_prototypes(dataSet_1_allTrainEmbed[i][j], prototypes)\n",
    "  return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "jlTDDaFIMy7A"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate(ind , prototypes1):\n",
    "  predict=[]\n",
    "  print(f\"Evaluation metrics for model {ind}\")\n",
    "  for i in range(ind):\n",
    "    for j in range(2500):\n",
    "      predict.append(pred(i ,j , prototypes1))\n",
    "    accuracy = accuracy_score(all_eval_targets[i], predict)\n",
    "    # Print the metrics\n",
    "    print(f\"for Dataset {i+1} Accuracy : {accuracy:.4f}\")\n",
    "    predict=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbxMvO_MBpDT",
    "outputId": "d442542b-d7a7-453b-be6a-14643b9d7746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for model 1\n",
      "for Dataset 1 Accuracy : 0.9620\n"
     ]
    }
   ],
   "source": [
    "evaluate(1, prototypes1_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHEExP7zPLb_",
    "outputId": "80dc7248-b031-4fe6-8e79-996489a766b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for model 2\n",
      "for Dataset 1 Accuracy : 0.9620\n",
      "for Dataset 2 Accuracy : 0.9524\n",
      "Evaluation metrics for model 3\n",
      "for Dataset 1 Accuracy : 0.9616\n",
      "for Dataset 2 Accuracy : 0.9512\n",
      "for Dataset 3 Accuracy : 0.9536\n",
      "Evaluation metrics for model 4\n",
      "for Dataset 1 Accuracy : 0.9616\n",
      "for Dataset 2 Accuracy : 0.9512\n",
      "for Dataset 3 Accuracy : 0.9536\n",
      "for Dataset 4 Accuracy : 0.9500\n",
      "Evaluation metrics for model 5\n",
      "for Dataset 1 Accuracy : 0.9620\n",
      "for Dataset 2 Accuracy : 0.9516\n",
      "for Dataset 3 Accuracy : 0.9536\n",
      "for Dataset 4 Accuracy : 0.9500\n",
      "for Dataset 5 Accuracy : 0.9528\n",
      "Evaluation metrics for model 6\n",
      "for Dataset 1 Accuracy : 0.9616\n",
      "for Dataset 2 Accuracy : 0.9516\n",
      "for Dataset 3 Accuracy : 0.9536\n",
      "for Dataset 4 Accuracy : 0.9496\n",
      "for Dataset 5 Accuracy : 0.9524\n",
      "for Dataset 6 Accuracy : 0.9500\n",
      "Evaluation metrics for model 7\n",
      "for Dataset 1 Accuracy : 0.9612\n",
      "for Dataset 2 Accuracy : 0.9516\n",
      "for Dataset 3 Accuracy : 0.9532\n",
      "for Dataset 4 Accuracy : 0.9492\n",
      "for Dataset 5 Accuracy : 0.9528\n",
      "for Dataset 6 Accuracy : 0.9496\n",
      "for Dataset 7 Accuracy : 0.9500\n",
      "Evaluation metrics for model 8\n",
      "for Dataset 1 Accuracy : 0.9612\n",
      "for Dataset 2 Accuracy : 0.9520\n",
      "for Dataset 3 Accuracy : 0.9536\n",
      "for Dataset 4 Accuracy : 0.9492\n",
      "for Dataset 5 Accuracy : 0.9528\n",
      "for Dataset 6 Accuracy : 0.9496\n",
      "for Dataset 7 Accuracy : 0.9496\n",
      "for Dataset 8 Accuracy : 0.9512\n",
      "Evaluation metrics for model 9\n",
      "for Dataset 1 Accuracy : 0.9608\n",
      "for Dataset 2 Accuracy : 0.9516\n",
      "for Dataset 3 Accuracy : 0.9528\n",
      "for Dataset 4 Accuracy : 0.9488\n",
      "for Dataset 5 Accuracy : 0.9528\n",
      "for Dataset 6 Accuracy : 0.9488\n",
      "for Dataset 7 Accuracy : 0.9496\n",
      "for Dataset 8 Accuracy : 0.9516\n",
      "for Dataset 9 Accuracy : 0.9508\n",
      "Evaluation metrics for model 10\n",
      "for Dataset 1 Accuracy : 0.9604\n",
      "for Dataset 2 Accuracy : 0.9516\n",
      "for Dataset 3 Accuracy : 0.9524\n",
      "for Dataset 4 Accuracy : 0.9488\n",
      "for Dataset 5 Accuracy : 0.9524\n",
      "for Dataset 6 Accuracy : 0.9488\n",
      "for Dataset 7 Accuracy : 0.9492\n",
      "for Dataset 8 Accuracy : 0.9520\n",
      "for Dataset 9 Accuracy : 0.9508\n",
      "for Dataset 10 Accuracy : 0.9572\n"
     ]
    }
   ],
   "source": [
    "prototypes = prototypes1_new.copy()\n",
    "for i in range(1,10):\n",
    "    predict_target = []\n",
    "    for j in range(2500):\n",
    "      predict_target.append(pred1(i , j , prototypes))\n",
    "    predict_target = np.array(predict_target)\n",
    "    new_mean  = mean(dataSet_1_allTrainEmbed[i]  , predict_target)\n",
    "    for k in range(10):\n",
    "      prototypes[k] = (0.9 * prototypes[k] + 0.1 * new_mean[k])\n",
    "    evaluate(i+1, prototypes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Dd7xQEK5ThPU",
    "outputId": "806c0aeb-2ba8-455c-ba8e-541fe74bfce2"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_f6a2ecfb-9f47-46f8-a79c-f7413f441b8d\", \"mean_after_1.1.pkl\", 41441)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save dictionary to a file\n",
    "with open('mean_after_1.1.pkl', 'wb') as file:\n",
    "    pickle.dump(prototypes, file)\n",
    "from google.colab import files\n",
    "\n",
    "files.download('mean_after_1.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "hgQy7EVeA0a3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
